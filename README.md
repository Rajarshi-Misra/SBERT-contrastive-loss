# SBERT-contrastive-loss
## Contrastive Learning for Sentence Embeddings with SBERT

## Overview:
This project explored multiple variants of supervised contrastive loss functions applied to sentence embeddings generated by SBERT (Sentence-BERT) models. The goal was to improve sentence classification performance and gain deeper insights into the quality of the learned representations.

## Key Contributions:

- Loss Function Exploration: Implemented and compared four contrastive loss variants:

- Cosine similarity loss

- Supervised contrastive loss (custom and library-based versions)

- Contrastive loss with hard negatives

- Dataset Evaluation: Tested on benchmark datasets including MR, R8, and R52.

- Optimization: Used Optuna (Bayesian hyperparameter search) to tune learning rates, batch size, and other hyperparameters.

## Experimental Insights:

- Analyzed data distribution with t-SNE plots and cosine similarity heatmaps

- Tracked representation drift during training through epoch-wise similarity analysis

- Investigated eigenvalue decay of embedding covariance matrices to assess representation quality using the power-law relationship

## Results:

- Achieved up to 91.35% accuracy on the MR dataset with improved supervised contrastive loss.

- Demonstrated consistent improvements over baseline SBERT + MLP setups.

- Eigenvalue analysis showed that better separation of clusters correlates with higher leading eigenvalues.

## Tools & Libraries:

- PyTorch, SentenceTransformers

- Optuna for hyperparameter tuning

- scikit-learn (PCA), Seaborn/Matplotlib for visualization
